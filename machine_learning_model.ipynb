{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MachineLearningModel.ipynb\n",
    "In this notebook we will create a machine learning model using for Texture Recognition using Haralick Texture Features  \n",
    "\n",
    "\n",
    "Authors: Alexander Goudemond (219030365) and Sumeith Ishwanthlal (219006284)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from os import getcwd, walk\n",
    "from mahotas import features\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_haralick_features(image):\n",
    "        textures = features.haralick(image)\n",
    "        hf_mean = textures.mean(axis=0)\n",
    "        return hf_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_average_pixel_value(image):\n",
    "    return np.array([np.average(image[:, :, 0]), #average red \n",
    "                    np.average(image[:, :, 1]), #average green\n",
    "                    np.average(image[:, :, 2]), #average blue\n",
    "                    np.average(image)]) #average pixel value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started to extract features from images\n",
      "Processed 5 images...\n",
      "Processed 10 images...\n",
      "Processed 15 images...\n",
      "Processed 20 images...\n",
      "Processed 25 images...\n",
      "Processed 30 images...\n",
      "Processed 35 images...\n",
      "Processed 40 images...\n",
      "Processed 45 images...\n",
      "Processed 50 images...\n",
      "Processed 55 images...\n"
     ]
    }
   ],
   "source": [
    "train_features_haralick = []\n",
    "train_average_pixel = []\n",
    "train_images = []\n",
    "train_labels = []\n",
    "cnn_train_labels = []\n",
    "haralick_features_R10 = []\n",
    "haralick_features_R20 = []\n",
    "haralick_features_R50 = []\n",
    "haralick_features_R100 = []\n",
    "haralick_features_R200 = []\n",
    "\n",
    "average_pixel__R10 = []\n",
    "average_pixel__R20 = []\n",
    "average_pixel__R50 = []\n",
    "average_pixel__R100 = []\n",
    "average_pixel__R200 = []\n",
    "\n",
    "\n",
    "currentDir = getcwd()\n",
    "photoPath = currentDir + \"\\\\Resized_Notes_DataSet\"\n",
    "path = walk(photoPath)\n",
    "numProcessedImages = 0\n",
    "print(\"Started to extract features from images\")\n",
    "for root, directories, files in path:\n",
    "    for file in files:\n",
    "\n",
    "        greyimage = cv2.imread(\"Resized_Notes_DataSet\" + \"\\\\\" + file, cv2.IMREAD_GRAYSCALE)\n",
    "        colImage = cv2.imread(\"Resized_Notes_DataSet\" + \"\\\\\" + file, cv2.IMREAD_COLOR)\n",
    "\n",
    "        haralick_features = extract_haralick_features(greyimage)\n",
    "        train_features_haralick.append(haralick_features)\n",
    "        train_images.append(colImage)\n",
    "\n",
    "        average_pixel = extract_average_pixel_value(colImage)\n",
    "        train_average_pixel.append(average_pixel)\n",
    "\n",
    "        if \"010\" in file.split(\"_\"):\n",
    "            train_labels.append(\"R10\") \n",
    "            cnn_train_labels.append(0)\n",
    "            haralick_features_R10.append(haralick_features)\n",
    "            average_pixel__R10.append(average_pixel)\n",
    "            \n",
    "        if \"020\" in file.split(\"_\"):\n",
    "            train_labels.append(\"R20\")\n",
    "            cnn_train_labels.append(1)\n",
    "            haralick_features_R20.append(haralick_features)\n",
    "            average_pixel__R20.append(average_pixel)\n",
    "\n",
    "        if \"050\" in file.split(\"_\"):\n",
    "            train_labels.append(\"R50\")\n",
    "            cnn_train_labels.append(2)\n",
    "            haralick_features_R50.append(haralick_features)\n",
    "            average_pixel__R50.append(average_pixel)\n",
    "        \n",
    "        if \"100\" in file.split(\"_\"):\n",
    "            train_labels.append(\"R100\")\n",
    "            cnn_train_labels.append(3)\n",
    "            haralick_features_R100.append(haralick_features)\n",
    "            average_pixel__R100.append(average_pixel)\n",
    "        \n",
    "        if \"200\" in file.split(\"_\"):\n",
    "            train_labels.append(\"R200\")\n",
    "            cnn_train_labels.append(4)\n",
    "            haralick_features_R200.append(haralick_features)\n",
    "            average_pixel__R200.append(average_pixel)\n",
    "        \n",
    "        numProcessedImages += 1\n",
    "        if numProcessedImages % 5 == 0:\n",
    "            print(f\"Processed {numProcessedImages} images...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 Training features: \n",
      "[array([ 2.21735310e-03,  2.44882308e+01,  9.89739846e-01,  1.19330041e+03,\n",
      "        4.62530546e-01,  3.26819293e+02,  4.74871340e+03,  7.87819762e+00,\n",
      "        1.04907269e+01,  7.74987469e-04,  3.05994005e+00, -4.82180924e-01,\n",
      "        9.99256553e-01]), array([ 1.20108062e-03,  1.13388904e+02,  9.62949707e-01,  1.53006977e+03,\n",
      "        3.14995920e-01,  3.26316145e+02,  6.00689018e+03,  8.07467024e+00,\n",
      "        1.17076166e+01,  3.97791456e-04,  4.01307517e+00, -3.51088010e-01,\n",
      "        9.96104340e-01]), array([ 7.64297360e-04,  7.87853363e+01,  9.77069865e-01,  1.71774336e+03,\n",
      "        3.24528752e-01,  3.59658595e+02,  6.79218809e+03,  8.22449889e+00,\n",
      "        1.16125961e+01,  4.67528431e-04,  3.73512105e+00, -3.95202558e-01,\n",
      "        9.98115093e-01]), array([ 6.19590581e-04,  2.97784925e+02,  9.11337101e-01,  1.67917418e+03,\n",
      "        1.85281168e-01,  3.25753995e+02,  6.41891179e+03,  8.08981896e+00,\n",
      "        1.25459982e+01,  2.15247560e-04,  4.79594544e+00, -2.41134960e-01,\n",
      "        9.82820798e-01]), array([ 7.04364555e-03,  3.55716222e+01,  9.91564336e-01,  2.10816158e+03,\n",
      "        4.33767172e-01,  3.98673602e+02,  8.39707471e+03,  7.78679370e+00,\n",
      "        1.04702533e+01,  6.07172952e-04,  3.33641013e+00, -4.68989114e-01,\n",
      "        9.99070062e-01])]\n"
     ]
    }
   ],
   "source": [
    "print(f\"The first 5 Training features: \\n{train_features_haralick[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 Training average pixels: \n",
      "[array([149.44164276, 179.03445625, 138.08300591, 155.51970164]), array([149.1907959 , 178.79446983, 137.83278275, 155.27268283]), array([157.14299011, 193.58179092, 161.51134682, 170.74537595]), array([148.86021233, 178.54326439, 137.53833199, 154.9806029 ]), array([182.90674973, 206.52408028, 191.63935089, 193.6900603 ])]\n"
     ]
    }
   ],
   "source": [
    "print(f\"The first 5 Training average pixels: \\n{train_average_pixel[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first training image: \n",
      "[[[171 203 138]\n",
      "  [170 202 137]\n",
      "  [164 198 128]\n",
      "  ...\n",
      "  [168 201 127]\n",
      "  [171 204 130]\n",
      "  [172 205 131]]\n",
      "\n",
      " [[171 203 138]\n",
      "  [169 201 136]\n",
      "  [163 197 127]\n",
      "  ...\n",
      "  [167 200 126]\n",
      "  [170 203 129]\n",
      "  [171 204 130]]\n",
      "\n",
      " [[168 200 135]\n",
      "  [167 199 134]\n",
      "  [161 195 125]\n",
      "  ...\n",
      "  [166 199 125]\n",
      "  [169 202 128]\n",
      "  [170 203 129]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[168 203 123]\n",
      "  [167 202 122]\n",
      "  [158 193 113]\n",
      "  ...\n",
      "  [164 197 122]\n",
      "  [168 201 126]\n",
      "  [169 202 127]]\n",
      "\n",
      " [[170 204 127]\n",
      "  [170 204 127]\n",
      "  [161 196 116]\n",
      "  ...\n",
      "  [166 199 124]\n",
      "  [170 203 128]\n",
      "  [171 204 129]]\n",
      "\n",
      " [[171 205 128]\n",
      "  [171 205 128]\n",
      "  [162 196 119]\n",
      "  ...\n",
      "  [167 200 125]\n",
      "  [171 204 129]\n",
      "  [172 205 130]]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"The first training image: \\n{train_images[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 Training labels: ['R10', 'R10', 'R10', 'R10', 'R10']\n"
     ]
    }
   ],
   "source": [
    "print(f\"The first 5 Training labels: {train_labels[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_train_labels = to_categorical(cnn_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_train_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2b9c5593eb0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x= [ [1, 11, 21, 31] * 11,[2, 12, 22, 32] * 11 ,[3, 13, 23, 33] * 11, [4, 14, 24, 34] * 11, [5, 15, 25, 35] * 11],\n",
    "y = [average_pixel__R10,average_pixel__R20,average_pixel__R50,average_pixel__R100,average_pixel__R200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Machine Learning Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sumei\\anaconda3\\envs\\COMP702Proj\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(max_iter=100000000, random_state=9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelHaralick = LinearSVC(random_state = 9, max_iter = 100000000)\n",
    "c.fit(train_features_haralick, train_labels)\n",
    "filename = 'modelHaralick.sav'\n",
    "pickle.dump(modelHaralick, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(max_iter=100000000, random_state=9)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelAveragePixel = LinearSVC(random_state = 9, max_iter = 100000000)\n",
    "modelAveragePixel.fit(train_average_pixel, train_labels)\n",
    "filename = 'modelAveragePixel.sav'\n",
    "pickle.dump(modelAveragePixel, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 1024, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 510, 1022, 64)     1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 255, 511, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 253, 509, 132)     76164     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 126, 254, 132)     0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4224528)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                270369856 \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 270,448,137\n",
      "Trainable params: 270,448,137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn = models.Sequential([\n",
    "    layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', input_shape=(512, 1024, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(filters=132, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(5, activation='softmax')\n",
    "])\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512\n  y sizes: 55\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sumei\\Documents\\UKZN\\Honours\\Semester 1\\COMP702\\other 3\\COMP702_CourseProject\\machine_learning_model.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sumei/Documents/UKZN/Honours/Semester%201/COMP702/other%203/COMP702_CourseProject/machine_learning_model.ipynb#ch0000018?line=0'>1</a>\u001b[0m cnn\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sumei/Documents/UKZN/Honours/Semester%201/COMP702/other%203/COMP702_CourseProject/machine_learning_model.ipynb#ch0000018?line=1'>2</a>\u001b[0m cnn\u001b[39m.\u001b[39;49mfit(train_images, cnn_train_labels, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\sumei\\anaconda3\\envs\\COMP702Proj\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1143\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cluster_coordinator \u001b[39m=\u001b[39m cluster_coordinator\u001b[39m.\u001b[39mClusterCoordinator(\n\u001b[0;32m   1138\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy)\n\u001b[0;32m   1140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy\u001b[39m.\u001b[39mscope(), \\\n\u001b[0;32m   1141\u001b[0m      training_utils\u001b[39m.\u001b[39mRespectCompiledTrainableState(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1142\u001b[0m   \u001b[39m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[1;32m-> 1143\u001b[0m   data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[0;32m   1144\u001b[0m       x\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m   1145\u001b[0m       y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m   1146\u001b[0m       sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1147\u001b[0m       batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1148\u001b[0m       steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   1149\u001b[0m       initial_epoch\u001b[39m=\u001b[39;49minitial_epoch,\n\u001b[0;32m   1150\u001b[0m       epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m   1151\u001b[0m       shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   1152\u001b[0m       class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[0;32m   1153\u001b[0m       max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1154\u001b[0m       workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1155\u001b[0m       use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1156\u001b[0m       model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1157\u001b[0m       steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution)\n\u001b[0;32m   1159\u001b[0m   \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mc:\\Users\\sumei\\anaconda3\\envs\\COMP702Proj\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1400\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1398\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(kwargs[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m_cluster_coordinator\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1399\u001b[0m   \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 1400\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sumei\\anaconda3\\envs\\COMP702Proj\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1155\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1152\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution_value \u001b[39m=\u001b[39m steps_per_execution\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m   1154\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1155\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[0;32m   1156\u001b[0m     x,\n\u001b[0;32m   1157\u001b[0m     y,\n\u001b[0;32m   1158\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1159\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   1160\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[0;32m   1161\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1162\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   1163\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1164\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1165\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1166\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mds_context\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[0;32m   1167\u001b[0m     model\u001b[39m=\u001b[39;49mmodel)\n\u001b[0;32m   1169\u001b[0m strategy \u001b[39m=\u001b[39m ds_context\u001b[39m.\u001b[39mget_strategy()\n\u001b[0;32m   1171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\sumei\\anaconda3\\envs\\COMP702Proj\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:258\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    255\u001b[0m inputs \u001b[39m=\u001b[39m pack_x_y_sample_weight(x, y, sample_weights)\n\u001b[0;32m    257\u001b[0m num_samples \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\u001b[39mint\u001b[39m(i\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m nest\u001b[39m.\u001b[39mflatten(inputs))\u001b[39m.\u001b[39mpop()\n\u001b[1;32m--> 258\u001b[0m _check_data_cardinality(inputs)\n\u001b[0;32m    260\u001b[0m \u001b[39m# If batch_size is not passed but steps is, calculate from the input data.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[39m# Default to 32 for backwards compat.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batch_size:\n",
      "File \u001b[1;32mc:\\Users\\sumei\\anaconda3\\envs\\COMP702Proj\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1666\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1663\u001b[0m   msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m sizes: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1664\u001b[0m       label, \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(i\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m nest\u001b[39m.\u001b[39mflatten(single_data)))\n\u001b[0;32m   1665\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1666\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512\n  y sizes: 55\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "cnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "cnn.fit(train_images, cnn_train_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "listA = []\n",
    "for root, directories, files in path:\n",
    "    for file in files:\n",
    "        image = cv2.imread(\"Notes_DataSet\\\\\" + file, cv2.IMREAD_GRAYSCALE)\n",
    "        haralick_features = extract_features(image)\n",
    "        prediction = SVCmodel.predict(features.reshape(1, -1))[0]\n",
    "        print(haralick_features)\n",
    "        listA.append(prediction)\n",
    "\n",
    "print(listA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('COMP702Proj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f9485475195dc84a11a6143ed063eb4083972a273d23aac80d9d84c42b96105"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
